{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZohaibCodez/openai-agents-chainlit-lab/blob/main/01_hello_agent/async/hello_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUVPp9cmsmpc"
      },
      "source": [
        "# Running Google's Gemini using run with OpenAI Agents SDK in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F16KTq4RsyNx"
      },
      "source": [
        "## Installing OpenAI Agents SDk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eMo8xvUsrie",
        "outputId": "080dffb8-e5e5-44af-9ab1-a9d2b35bc5e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB4-s1oDsuH5"
      },
      "source": [
        "## Making Jupyter Notebook capable of running asynchronous functions like our RunLoopEvent(Runner.run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMddCPyxs3gJ"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36OVBSUzs56P"
      },
      "source": [
        "## Running Google's GEMINI with OpenAI Agents SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Fbq7Eces8yB"
      },
      "outputs": [],
      "source": [
        "from agents import Agent,Runner,AsyncOpenAI,OpenAIChatCompletionsModel,RunResult\n",
        "from agents.run import RunConfig\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW95w-1-s-oU"
      },
      "outputs": [],
      "source": [
        "gemini_api_key = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not gemini_api_key:\n",
        "  raise ValueError(\"GEMINI_API_KEY is not set. Please ensure it is set\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "    )\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    openai_client = external_client\n",
        ")\n",
        "\n",
        "run_config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=external_client,\n",
        "    tracing_disabled=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFFIvZv0tc6O"
      },
      "source": [
        "## Hello World Code | Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97sMQWDXthbW",
        "outputId": "f45e3821-a9d3-4253-e1bb-bfb6bcea122e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Zohaib! It's nice to meet you. How can I help you today?\n",
            "\n",
            "Your name is Zohaib.\n",
            "\n",
            "[MessageOutputItem(agent=Agent(name='Assistant', instructions='You are an agent - please keep going until the user’s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7aac7d738c10>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Your name is Zohaib.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item')]\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "  agent:Agent = Agent(name=\"Assistant\",instructions = \"You are an agent - please keep going until the user’s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.\",model=model)\n",
        "\n",
        "  result:RunResult =await Runner.run(starting_agent=agent,input=\"Hello...! My name is Zohaib\",run_config=run_config)\n",
        "  print(result.final_output)\n",
        "  # print(result.to_input_list())\n",
        "  new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": \"What is my name?\"}]\n",
        "  result2:RunResult = await Runner.run(starting_agent=agent,input=new_input,run_config=run_config)\n",
        "  print(result2.final_output)\n",
        "\n",
        "asyncio.run(main())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}